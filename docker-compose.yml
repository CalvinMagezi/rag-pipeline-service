version: '3.8'

services:
  rag-api:
    build:
      context: .
      dockerfile: apps/rag-api/Dockerfile
    container_name: rag-api
    ports:
      - "8888:8888"
    environment:
      # Server config
      NODE_ENV: production
      PORT: 8888
      HOST: 0.0.0.0

      # Vector store config
      VECTOR_STORE_PROVIDER: filesystem
      VECTOR_STORE_PATH: /app/data/vectors

      # Gemini config (active)
      EMBEDDING_PROVIDER: gemini
      EMBEDDING_DIMENSION: 768  
      GEMINI_API_KEY: ${GEMINI_API_KEY}
      GEMINI_MODEL: text-embedding-004

      # OpenAI config (alternative - comment Gemini and uncomment these)
      # EMBEDDING_PROVIDER: openai
      # EMBEDDING_DIMENSION: 1536
      # OPENAI_API_KEY: ${OPENAI_API_KEY}
      # EMBEDDING_MODEL: text-embedding-3-small

      # Chunking config
      CHUNKING_STRATEGY: recursive
      CHUNK_SIZE: 512
      CHUNK_OVERLAP: 50

      # Query config
      QUERY_TOP_K: 5
      QUERY_MIN_SCORE: 0.7
    volumes:
      # Persist vector data
      - rag-data:/app/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8888/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  web-app:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
    container_name: rag-web-app
    ports:
      - "3333:3333"
    environment:
      # Server config
      NODE_ENV: production
      PORT: 3333
      HOSTNAME: 0.0.0.0
      
      # RAG API connection
      RAG_API_URL: http://rag-api:8888
    depends_on:
      - rag-api
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3333/api/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

volumes:
  rag-data:
    driver: local
